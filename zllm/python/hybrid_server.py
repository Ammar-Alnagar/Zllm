# hybrid_server.py - Hybrid Mini-vLLM server with real transformers inference

import asyncio
import time
import uuid
from typing import List, Dict, Optional, Any
from contextlib import asynccontextmanager

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import uvicorn
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# Import Mini-vLLM components for infrastructure
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from config import ModelConfig, InferenceConfig


# Pydantic models for API
class GenerateRequest(BaseModel):
    """Request model for text generation"""

    prompt: str = Field(..., description="Input text prompt")
    max_tokens: Optional[int] = Field(50, description="Maximum tokens to generate")
    temperature: Optional[float] = Field(0.8, description="Sampling temperature")
    top_p: Optional[float] = Field(0.95, description="Top-p (nucleus) sampling")
    top_k: Optional[int] = Field(50, description="Top-k sampling (-1 = disabled)")


class GenerateResponse(BaseModel):
    """Response model for text generation"""

    id: str = Field(..., description="Request ID")
    object: str = Field("text_completion", description="Object type")
    created: int = Field(..., description="Creation timestamp")
    model: str = Field("mini_vllm", description="Model name")
    choices: List[Dict[str, Any]] = Field(..., description="Generated completions")
    usage: Dict[str, int] = Field(..., description="Token usage statistics")


class HealthResponse(BaseModel):
    """Health check response"""

    status: str = Field("ok", description="Service status")
    version: str = Field("0.1.0", description="Service version")
    cuda_available: bool = Field(False, description="CUDA availability")
    model_loaded: bool = Field(False, description="Model loaded status")


# Global instances
tokenizer = None
model = None
model_loaded = False


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan manager"""
    global tokenizer, model, model_loaded

    print("üöÄ Starting Hybrid Mini-vLLM server...")
    print("üìù Using transformers for inference, Mini-vLLM for infrastructure")

    try:
        # Load tokenizer (Mini-vLLM style)
        print("Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen3-0.6B")
        print("‚úì Tokenizer loaded")

        # Load model with transformers for actual inference
        print("Loading Qwen/Qwen3-0.6B model with transformers...")
        model = AutoModelForCausalLM.from_pretrained(
            "Qwen/Qwen3-0.6B",
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
        )
        print("‚úì Model loaded successfully")

        model_loaded = True
        print("üéâ Hybrid Mini-vLLM server ready!")
        print("üîÑ Using transformers inference with Mini-vLLM request handling")

    except Exception as e:
        print(f"‚úó Failed to initialize: {e}")
        import traceback

        traceback.print_exc()
        model_loaded = False

    yield

    print("Shutting down Hybrid Mini-vLLM server...")
    if model:
        del model
    tokenizer = None
    model_loaded = False


# Create FastAPI app
app = FastAPI(
    title="Hybrid Mini-vLLM API",
    description="Mini-vLLM infrastructure with transformers inference",
    version="0.1.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    return HealthResponse(
        status="ok" if model_loaded else "error",
        version="0.1.0",
        cuda_available=torch.cuda.is_available(),
        model_loaded=model_loaded,
    )


@app.post("/generate")
async def generate_text(request: GenerateRequest):
    """Real text generation using transformers with Mini-vLLM infrastructure"""

    if not model_loaded or model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        # Tokenize input (Mini-vLLM style tokenization)
        inputs = tokenizer(request.prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = inputs.to("cuda")

        # Generate with transformers (real inference)
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens or 50,
                temperature=request.temperature or 0.8,
                top_p=request.top_p or 0.95,
                top_k=request.top_k or 50,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )

        # Decode output (Mini-vLLM style)
        generated_text = tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]) :], skip_special_tokens=True
        )

        # Calculate token counts
        input_tokens = len(inputs["input_ids"][0])
        output_tokens = len(outputs[0]) - input_tokens

        return {
            "generated_text": generated_text,
            "num_tokens": output_tokens,
            "model": "Qwen/Qwen3-0.6B",
            "inference_engine": "transformers (with Mini-vLLM infrastructure)",
            "parameters": {
                "temperature": request.temperature or 0.8,
                "top_p": request.top_p or 0.95,
                "top_k": request.top_k or 50,
                "max_tokens": request.max_tokens or 50,
            },
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


@app.post("/v1/completions", response_model=GenerateResponse)
async def create_completion(request: GenerateRequest):
    """OpenAI-compatible completions with real inference"""

    if not model_loaded or model is None or tokenizer is None:
        raise HTTPException(status_code=503, detail="Model not loaded")

    try:
        # Generate unique request ID
        request_id = str(uuid.uuid4())

        # Tokenize and generate
        inputs = tokenizer(request.prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = inputs.to("cuda")

        start_time = time.time()
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=request.max_tokens or 50,
                temperature=request.temperature or 0.8,
                top_p=request.top_p or 0.95,
                top_k=request.top_k or 50,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
            )
        end_time = time.time()

        # Decode and format response
        generated_text = tokenizer.decode(
            outputs[0][len(inputs["input_ids"][0]) :], skip_special_tokens=True
        )

        # Calculate usage stats
        input_tokens = len(inputs["input_ids"][0])
        output_tokens = len(outputs[0]) - input_tokens

        response = GenerateResponse(
            id=request_id,
            object="text_completion",
            created=int(start_time),
            model="qwen3-0.6b-hybrid-minivllm",
            choices=[
                {
                    "text": generated_text,
                    "index": 0,
                    "logprobs": None,
                    "finish_reason": "length",
                }
            ],
            usage={
                "prompt_tokens": input_tokens,
                "completion_tokens": output_tokens,
                "total_tokens": input_tokens + output_tokens,
            },
        )

        return response

    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Inference failed: {str(e)}")


@app.get("/info")
async def system_info():
    """Get system and model information"""
    if not model_loaded:
        return {"status": "model_not_loaded"}

    return {
        "system": {
            "cuda_available": torch.cuda.is_available(),
            "gpu_name": torch.cuda.get_device_name()
            if torch.cuda.is_available()
            else None,
            "pytorch_version": torch.__version__,
        },
        "model": {
            "name": "Qwen/Qwen3-0.6B",
            "architecture": "Hybrid (Mini-vLLM + transformers)",
            "parameters": "~0.6B",
            "inference": "transformers.generate()",
            "tokenization": "Mini-vLLM compatible",
        },
        "features": [
            "Real text generation (not mock)",
            "OpenAI-compatible API",
            "Temperature/top-p/top-k sampling",
            "CUDA acceleration when available",
            "Mini-vLLM infrastructure",
        ],
    }


def main():
    """Main entry point"""
    import argparse

    parser = argparse.ArgumentParser(description="Hybrid Mini-vLLM Server")
    parser.add_argument("--host", default="0.0.0.0", help="Host to bind to")
    parser.add_argument("--port", type=int, default=8000, help="Port to bind to")

    args = parser.parse_args()

    print("üöÄ Starting Hybrid Mini-vLLM Server")
    print("üîÑ Mini-vLLM infrastructure + transformers inference")
    print(f"üìç Host: {args.host}:{args.port}")

    uvicorn.run("hybrid_server:app", host=args.host, port=args.port, reload=False)


if __name__ == "__main__":
    main()
