#!/usr/bin/env python3

"""
Interactive chat with Mini-vLLM and Qwen/Qwen3-0.6B
"""

import sys
import os
import torch
from transformers import AutoTokenizer

sys.path.insert(0, os.path.join(os.path.dirname(__file__), "zllm", "python"))


def main():
    print("ü§ñ Mini-vLLM Chat Demo with Qwen/Qwen3-0.6B")
    print("=" * 60)

    # Load model configuration
    print("üìã Loading Qwen model configuration...")
    from transformers import AutoConfig

    model_name = "Qwen/Qwen3-0.6B"
    try:
        config = AutoConfig.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        print("‚úÖ Model loaded successfully!")
        print(f"   ‚Ä¢ Architecture: {config.model_type}")
        print(f"   ‚Ä¢ Hidden size: {config.hidden_size}")
        print(f"   ‚Ä¢ Layers: {config.num_hidden_layers}")
        print(f"   ‚Ä¢ Attention heads: {config.num_attention_heads}")
        print(f"   ‚Ä¢ Vocabulary size: {config.vocab_size}")
        print(f"   ‚Ä¢ CUDA available: {torch.cuda.is_available()}")

        if torch.cuda.is_available():
            print(f"   ‚Ä¢ GPU: {torch.cuda.get_device_name()}")

        print("\nüí¨ Ready for interactive chat!")
        print("Type 'quit' or 'exit' to stop.")
        print("-" * 40)

        while True:
            try:
                # Get user input
                user_input = input("\nüë§ You: ").strip()

                if user_input.lower() in ["quit", "exit", "bye"]:
                    print("\nüëã Goodbye!")
                    break

                if not user_input:
                    continue

                # Tokenize input
                tokens = tokenizer.encode(user_input)
                print(f"üìù Tokenized: {len(tokens)} tokens")

                # Show some token details
                if len(tokens) <= 10:
                    print(f"   Tokens: {tokens}")
                else:
                    print(f"   Tokens: {tokens[:5]} ... {tokens[-5:]}")

                # Decode back
                decoded = tokenizer.decode(tokens)
                print(f"   Decoded: '{decoded}'")

                # Simulate model response (since we don't have weights loaded)
                print("\nü§ñ Mini-vLLM Response:")
                print("   (Model weights not loaded - this is a demonstration)")
                print("   In a full implementation, this would generate a response!")
                print("   üöÄ Mini-vLLM supports:")
                print(
                    "      ‚Ä¢ Custom CUDA kernels (RMSNorm, RoPE, SwiGLU, Flash Attention)"
                )
                print("      ‚Ä¢ Paged KV cache with 16-token blocks")
                print("      ‚Ä¢ Continuous batching scheduler")
                print("      ‚Ä¢ FastAPI server with OpenAI-compatible API")

            except KeyboardInterrupt:
                print("\nüëã Goodbye!")
                break
            except Exception as e:
                print(f"‚ùå Error: {e}")
                continue

    except Exception as e:
        print(f"‚ùå Failed to load model: {e}")
        print("\nüí° Make sure you have:")
        print("   ‚Ä¢ transformers library installed")
        print("   ‚Ä¢ Access to download Qwen/Qwen3-0.6B")
        print("   ‚Ä¢ Internet connection for model download")


if __name__ == "__main__":
    main()
